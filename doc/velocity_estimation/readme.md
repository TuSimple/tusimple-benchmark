# TuSimple Velocity Estimation Challenge

![](https://raw.githubusercontent.com/TuSimple/tusimple-benchmark/master/doc/velocity_estimation/assets/examples/dis1.jpeg)



The goal of this challenge is to estimate the motion and position of vehicles relative to the camera. 

To perceive the dynamic surroundings of the autonomous vehicle is a critical task to achieve autonomous driving. The information about the position, as well as the motion of the agents in the vehicle’s surroundings plays an important role in motion planning. Traditionally, such information is perceived by an expensive range sensor, e.g LiDAR or MMW radar. In this challenge, we provide a challenge task to encourage people to think creatively about how to solve the velocity and position estimation problem by using only information from cameras. 
	
In this challenge, the participants are required to develop algorithms to estimate the velocity and the position of designated vehicles on given test clip. A set of over 1000 2-second-long video clips are provided, with velocity and positions generated by range sensors of vehicles on the last frame. We also provide human-annotated bounding boxes for vehicles on over 5,000 images as supplementary training data. Apart from the training and supplementary data we provide, external data sources are also allowed to be used to help the algorithm’s learning process. 
 
We will have a leaderboard showing the evaluation results for submissions. We have prizes for the top three competitors, who will also be mentioned at the CVPR 2017 Workshop on Autonomous Driving Challenge.
## Dataset Feature
Complexity
- Daytime recorded video on highway
- Vehicles with relative distance ranging from 5 meters to up to 90 meters.
- Variable traffic conditions. 
	
	Size:
- Training: 1074 clips of 2s videos in 20 frames per second. 3222 annotated vehicles.
- Testing: 269 clips of videos with same format as training data.
- Supplementary data: 5066 images with human labeled bounding boxes on vehicles.
	
	Annotations:
Position and velocity generated by range sensors.

## Dataset Details

### Directory Structure:
`benchmark_velocity_train.zip`:
```
      |----readme.md                  # description
      |
      |----calibration.txt            #intrinsic parameters of the used camera
      |----clips/                     # 1074 video clips
      |------|----...
      |------|----some_clip/              # images and json labels for each clip
      |------|--------|
      |------|--------|----imgs/              # 40 frames of 20 fps video recorded, 2 seconds in total
      |------|--------|
      |------|--------|----annotation.json    # json annotation of designated vehicles.
      |------|----...
```
`benchmark_velocity_test.zip`:
```
      |----readme.md                  # description
      |
      |----calibration.txt            #intrinsic parameters of the used camera
      |----clips/                     # 269 video clips
      |------|----...
      |------|----some_clip/              # images and json labels for each clip
      |------|--------|
      |------|--------|----imgs/              # 40 frames of 20 fps video recorded, 2 seconds in total
      |------|--------|
      |------|--------|----annotation.json    # json annotation of designated vehicles.
```
`benchmark_velocity_supp.zip`:
```
      |----readme.md              # description
      |
      |----annotation.json      # bounding box annotations
      |----supp_img/               # 5066 images
```      

### Demo
The [demo code](https://github.com/TuSimple/tusimple-benchmark/blob/master/example/velocity_demo.ipynb) shows the data format of the velocity estimation dataset and the usage of the evaluation tool.      

### Label Data Format

`benchmark_velocity.zip`:
 - calibration.txt: a text file containing the intrinsic parameters of the camera used, structured in a 3*3 matrix and the camera height to the ground.

Each of the training clip can be found under a clip folder named after a integer.
The files in the folder are structured as follows:
 - imgs/:  subfolder contains 40 frames of images recorded at 20 fps, the end of the which, 040.jpg, is the frame that ground truth annotation on, and the frame that needs to estimate velocity on.
 - annotation.json: a json file, containing the ground truth velocity and position for designated vehicles.

And the ground truth json file is structured as follows:
```
{ 
   [vehicle]: an array of [vehicle], defining the velocity and position of each vehicle in the image.
}

vehicle:
{
  "bbox": a json structure with 4 fields 'top','left','bottom','right': The axis-aligned rectangle specifying the extent of the vehicle in the image.
  "velocity": a float pair [x,y]. Relative planar velocity of the vehicle in meters per second. x direction is the same with the camera optical axis and y direction is vertical to x and towards right.
  "position": a float pair [x,y]. Planar position of the nearest point on vehicle in meters. x direction is the same with the camera optical axis, and y direction is vertical to x and towards right.
}
```
`benchmark_velocity_test.zip`:
 - calibration.txt: a text file containing the intrinsic parameters of the camera used, structured in a 3*3 matrix and the camera height to the ground.

Each of the test clip can be found under a clip folder named after a integer.
The files in the folder are structured as follows:
 - imgs/:  subfolder contains 40 frames of images recorded at 20 fps, the end of the which, 040.jpg, is the frame that ground truth annotation on, and the frame that needs to estimate velocity on.
 - annotation.json: a json file, containing the bounding box for designated vehicles you need to estimate velocity and position on.

And the annotation json file is structured as follows:
```
{ 
   [bbox]: an array of [bbox], defining the position of each vehicle on the image.
}

bbox:
{
  "bbox": a json structure with 4 fields 'top','left','bottom','right': The axis-aligned rectangle specifying the extent of the vehicle in the image.
}
```
`benchmark_velocity_supp.zip`:
  - supp_img/: folder contains all the training images, named in 4 digit numbers.
  - annotation.json: a json file, containing the ground truth bounding box for all vehicles in all training images, structured as follows:
```
{
  [img]: an array of [img], representing the ground truth bounding box annotations for each image.
}

img:
{
  "file_name": a string representing the image file name.
  "bbox": a list of json structure [{'left', 'top', 'bottom', 'right'}], each representing the axis-aligned rectangle specifying the extend of a vehicle in this image.
}
```

## Evaluation
### Submission
Your submission should contain a single json file, structured as follows:
```
{
   [frame]: an array of your result for each frame, sorted in the same way with the order of testing clips.
}

frame:
{
   [vehicle]: an array of your result for each designated vehicle in a certain frame, formatted in a same structre with the ground-truth data.
}

vehicle:
{
     "bbox": a json structure with 4 fields 'top','left','bottom','right': The axis-aligned rectangle specifying the given bounding box.
     "velocity": a float pair [x,y]. Predicted relative planar velocity of the vehicle in meters per second. x direction is the same with the camera optical axis and y direction is vertical to x and towards right.
     "position": a float pair [x,y]. Predicted planar position of the nearest point on vehicle in meters. x direction is the same with the camera optical axis, and y direction is vertical to x and towards right.
}
```
For our competition, you are required to provide a per-vehicle running time and the computing environment configuration for your method along with your submission. This information will not be included in the ranking.

### Evaluation Metrics
The metric we use in evaluating velocity estimation is Mean Squared Velocity Error:

<img src="https://latex.codecogs.com/gif.latex?$$E_v&space;=&space;\frac{\sum_{c\in&space;C}\|V^{gt}_c-V^{est}_c\|^2}{|C|}$$"/>

with <img src="https://latex.codecogs.com/gif.latex?$C$"/> denotes the set of submitted results for each vehicle, <img src="https://latex.codecogs.com/gif.latex?$V^{gt}_c$"/> represents the ground truth velocity for a certain vehicle, and <img src="https://latex.codecogs.com/gif.latex?$V^{est}_c$"/> represents the estimated velocity for that vehicle. Similarly, we use Mean Squared Position Error to evaluate position esitimation: 

<img src="https://latex.codecogs.com/gif.latex?$$E_p&space;=&space;\frac{\sum_{c\in&space;C}\|P^{gt}_c-P^{est}_c\|^2}{|C|}$$"/>

<img src="https://latex.codecogs.com/gif.latex?$P^{gt}_c$"/> represents the ground truth position of the nearest point on a certain vehicle, and <img src="https://latex.codecogs.com/gif.latex?$P^{est}_c$"/> represents the estimated position of the nearest point on such vehicle.

We classifiy test vehicles by its relative distance to the camera into three classes: Near(0-20m), Medium(20-45m), and Far(45m+). We evaluate the performance seperately on these three classes, and average them together to get the final error:

<img src="https://latex.codecogs.com/gif.latex?E_{v}&space;=&space;\frac{E_{vfar}&plus;E_{vmed}&plus;E_{vnear}}{3}" title="E_{vtotal} = \frac{E_{vfar}+E_{vmed}+E_{vnear}}{3}" />

<img src="https://latex.codecogs.com/gif.latex?E_{p}&space;=&space;\frac{E_{pfar}&plus;E_{pmed}&plus;E_{pnear}}{3}" title="E_{p} = \frac{E_{pfar}+E_{pmed}+E_{pnear}}{3}" />

In our evaluation code and leaderboard, the error of velocity estimation and error of position estimation is abbreviated as EV and EP respectively. 
### Prizes
The prizes for the winners are following. Please review the [rules]() for conditions to receive a prize.

1. First place prize: $ 1000
2. Second place prize: $ 500
3. Third place prize: $ 250 

For our competition, we rank your algorithm only based on the total performance of velocity estimation. So the final ranking will be determined on Total Mean Squared Velocity Error only.
